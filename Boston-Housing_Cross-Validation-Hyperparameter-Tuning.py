# -*- coding: utf-8 -*-
"""Boston Housing Kaggle Challenge with Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CO3yD46TqN5lUFeJeK9kE9VBJVj-q4nW
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

boston = pd.read_csv("/content/Boston.csv")

# Printing first 5 records of the dataset
print(dataset.head(5))

boston.shape

boston.columns

data = pd.DataFrame(boston)
data.head(10)

data.describe()

data.info()

# Input Data
x = boston.drop('medv', axis=1)  # Assuming 'medv' is the target variable

# Output Data
y = boston['medv']


# splitting data to training and testing dataset.

#from sklearn.cross_validation import train_test_split
#the submodule cross_validation is renamed and deprecated to model_selection
from sklearn.model_selection import train_test_split

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size =0.2,
													random_state = 0)

print("xtrain shape : ", xtrain.shape)
print("xtest shape : ", xtest.shape)
print("ytrain shape : ", ytrain.shape)
print("ytest shape : ", ytest.shape)

# Fitting Multi Linear regression model to training model
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(xtrain, ytrain)

# predicting the test set results
y_pred = regressor.predict(xtest)

# Plotting Scatter graph to show the prediction
# results - 'ytrue' value vs 'y_pred' value
plt.scatter(ytest, y_pred, c = 'green')
plt.xlabel("Price: in $1000's")
plt.ylabel("Predicted value")
plt.title("True value vs predicted value : Linear Regression")
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error
mse = mean_squared_error(ytest, y_pred)
mae = mean_absolute_error(ytest,y_pred)
print("Mean Square Error : ", mse)
print("Mean Absolute Error : ", mae)

from sklearn import svm
from sklearn.svm import SVC
from sklearn.metrics import mean_absolute_percentage_error

model_SVR = svm.SVR()
model_SVR.fit(X_train,Y_train)
Y_pred = model_SVR.predict(X_valid)

print(mean_absolute_percentage_error(Y_valid, Y_pred))

from sklearn.ensemble import RandomForestRegressor

model_RFR = RandomForestRegressor(n_estimators=10)
model_RFR.fit(X_train, Y_train)
Y_pred = model_RFR.predict(X_valid)

mean_absolute_percentage_error(Y_valid, Y_pred)

from sklearn.linear_model import LinearRegression

model_LR = LinearRegression()
model_LR.fit(X_train, Y_train)
Y_pred = model_LR.predict(X_valid)

print(mean_absolute_percentage_error(Y_valid, Y_pred))

!pip3 install catboost

# This code is contributed by @amartajisce
from catboost import CatBoostRegressor
cb_model = CatBoostRegressor()
cb_model.fit(X_train, Y_train)
preds = cb_model.predict(X_valid)
from sklearn.metrics import r2_score
cb_r2_score=r2_score(Y_valid, preds)
cb_r2_score

from sklearn.metrics import r2_score

# Calculate the R-squared score
r2 = r2_score(ytest, y_pred)

print("R-squared score:", r2)

"""As per the result, our model is only 66.55% accurate. So, the prepared model is not very good for predicting housing prices. One can improve the prediction results using many other possible machine learning algorithms and techniques.

Here are a few further steps on how we can improve your model.

Feature Selection
Cross-Validation
Hyperparameter Tuning"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# Select the top k features based on their F-scores
k = 5  # You can choose the desired number of features
selector = SelectKBest(f_regression, k=k)
xtrain_selected = selector.fit_transform(xtrain, ytrain)
xtest_selected = selector.transform(xtest)

from sklearn.model_selection import cross_val_score

# Create and fit a linear regression model
regressor_cv = LinearRegression()

# Perform cross-validation and compute the R-squared score
cv_scores = cross_val_score(regressor_cv, xtrain_selected, ytrain, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-Validation R-squared Score: ", mean_cv_score)

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

# Standardize your features
scaler = StandardScaler()
xtrain_scaled = scaler.fit_transform(xtrain)
xtest_scaled = scaler.transform(xtest)

# Feature selection
k = 5  # You can choose the desired number of features
selector = SelectKBest(f_regression, k=k)
xtrain_selected = selector.fit_transform(xtrain_scaled, ytrain)
xtest_selected = selector.transform(xtest_scaled)

# Create and fit a linear regression model
regressor_cv = LinearRegression()

# Perform cross-validation and compute the R-squared score
cv_scores = cross_val_score(regressor_cv, xtrain_selected, ytrain, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-Validation R-squared Score: ", mean_cv_score)

# Hyperparameter Tuning (if you are using Ridge or Lasso)
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}
regressor_tuned = GridSearchCV(Ridge(), param_grid, cv=5)
regressor_tuned.fit(xtrain_selected, ytrain)
best_params = regressor_tuned.best_params_
print("Best Hyperparameters: ", best_params)